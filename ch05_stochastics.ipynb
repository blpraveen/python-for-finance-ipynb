{
  "metadata": {
    "name": "Stochastics"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Stochastics"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Nowadays, stochastics is one of the most important mathematical and numerical disciplines in finance. In the beginning of the modern area of finance, mainly in the 1970s and 1980s, the major goal of financial research has been to come up with closed-form solutions for e.g. option prices given a specific financial model. The requirements have drastically increased in recent years in that not only the correct valuation of single financial instruments is important to participants in the financial markets but also the consistent valuation of whole derivatives or other books. Similary, to come up with consistent risk measures across a whole financial institution, like Value-at-Risk and Credit Value Adjustments, is something that needs to take into account the whole book of the institution and all its counterparites. Such daunting tasks can only be tackled by flexible and efficient numerical methods. Therefore, stochastics in general and Monte Carlo simulation in particular have risen to such prominence recently."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This chapter introduces into the following topics from a `Python` perspective:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p><strong>random number generation</strong>: it all starts with (pseudo-) random numbers which build the basis for all simulation efforts; although quasi-random numbers, e.g. based on Sobol sequences, have gained some popularity in finance, pseudo-random numbers still seem to be the clear benchmark</p>\n</li>\n<li>\n<p><strong>simulation</strong>: in finance, two simulation tasks are or particular importance: simulation of <em>random variables</em> and of <em>stochastic processes</em></p>\n</li>\n<li>\n<p><strong>valuation</strong>: the two main disciplines when it comes to valuation are the valuation of derivatives with <em>European exercise</em> (at a specific date) and <em>American exercise</em> (over a specific time interval); there are also instruments with <em>Bermudan exercise</em>, i.e. exercise at a finite set of specific dates</p>\n</li>\n<li>\n<p><strong>risk measures</strong>: simulation lends itself pretty well when it comes to the calculation of risk measures like <em>Value-at-Risk</em>, <em>Credit-Value-at-Risk</em> and <em>Credit Value Adjustments</em></p>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Random Numbers"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Throught this chapter, to generate random numbers [9](ch05.html#idp16678896) we will work with the functions provided by the `numpy.random` sub-library."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [1]: import numpy.random as npr\n        import matplotlib.pyplot as plt",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For example, the `rand` function returns random numbers from the open interval $[0,1)$ in the shape provided as parameters to the function. The return object is an `ndarray` object."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [2]: npr.rand(10)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[2]: array([ 0.6229885 ,  0.52893413,  0.14372375,  0.3812653 ,  0.89335038,\n                0.9537212 ,  0.10243602,  0.75068805,  0.33778371,  0.78127587])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [3]: npr.rand(5, 5)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[3]: array([[ 0.1425681 ,  0.72062454,  0.72437268,  0.24603875,  0.00949402],\n               [ 0.55875116,  0.06384183,  0.22468029,  0.10250227,  0.55968932],\n               [ 0.96596363,  0.62975008,  0.30537491,  0.14736187,  0.39930675],\n               [ 0.17971746,  0.49222084,  0.5260957 ,  0.46572673,  0.56004658],\n               [ 0.96472362,  0.85128256,  0.89354035,  0.67731002,  0.10735513]\n        ])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Such numbers can be easily transformed to cover other intervals of the real line. For instance, if you want to generate random numbers from the interval $[a,b)=[5,10)$, you can transform the returned numbers from `rand` as follows."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [4]: a = 5.\n        b = 10.\n        npr.rand(10) * (b - a) + a",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[4]: array([ 5.03613161,  6.68216385,  8.23171203,  6.9690603 ,  8.00644664,\n                6.89716218,  7.03911903,  7.54994108,  6.85727192,  8.54232106])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This also works for multi-dimensional shapes due to `NumPy` broadcasting."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [5]: npr.rand(5, 5) * (b - a) + a",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[5]: array([[ 9.58490329,  8.29352902,  5.42887624,  7.91091507,  6.51073706],\n               [ 6.857276  ,  6.74438277,  5.08447743,  8.55698432,  9.69829119],\n               [ 8.43136173,  9.73133548,  5.60537311,  7.17935387,  7.4147146 ],\n               [ 8.46935074,  8.25951065,  9.46960565,  9.57665406,  7.18135269],\n               [ 7.86294082,  8.28030124,  7.5584213 ,  8.84969911,  5.47679243]\n        ])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Table 5-1 lists functions for generating _simple_ (pseudo-) random numbers. [10](ch05.html#idp16795344)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table id=\"simple_randoms\">\n<caption>Simple (Pseudo-) Random Numbers</caption>\n<thead><tr>\n<th>function</th>\n<th>parameters</th>\n<th>description</th>\n</tr></thead>\n<tbody>\n<tr>\n<td><p><code>rand</code></p></td>\n<td><p>d0, d1, …, dn</p></td>\n<td><p>random values in a given shape</p></td>\n</tr>\n<tr>\n<td><p><code>randn</code></p></td>\n<td><p>d0, d1, …, dn</p></td>\n<td><p>a sample (or samples) from the standard normal distribution</p></td>\n</tr>\n<tr>\n<td><p><code>randint</code></p></td>\n<td><p>low[, high, size]</p></td>\n<td><p>random integers from low (inclusive) to high (exclusive)</p></td>\n</tr>\n<tr>\n<td><p><code>random_integers</code></p></td>\n<td><p>low[, high, size]</p></td>\n<td><p>random integers between low and high, inclusive</p></td>\n</tr>\n<tr>\n<td><p><code>random_sample</code></p></td>\n<td><p>[size]</p></td>\n<td><p>random floats in the half-open interval [0.0, 1.0)</p></td>\n</tr>\n<tr>\n<td><p><code>random</code></p></td>\n<td><p>[size]</p></td>\n<td><p>random floats in the half-open interval [0.0, 1.0)</p></td>\n</tr>\n<tr>\n<td><p><code>ranf</code></p></td>\n<td><p>[size]</p></td>\n<td><p>random floats in the half-open interval [0.0, 1.0)</p></td>\n</tr>\n<tr>\n<td><p><code>sample</code></p></td>\n<td><p>[size]</p></td>\n<td><p>random floats in the half-open interval [0.0, 1.0)</p></td>\n</tr>\n<tr>\n<td><p><code>choice</code></p></td>\n<td><p>a[, size, replace, p]</p></td>\n<td><p>random sample from a given 1-D array</p></td>\n</tr>\n<tr>\n<td><p><code>bytes</code></p></td>\n<td><p>length</p></td>\n<td><p>random bytes</p></td>\n</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let us visualize random draws generated by different functions."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [6]: sample_size = 500\n        rn1 = npr.rand(sample_size, 3)\n        rn2 = npr.randint(0, 10, sample_size)\n        rn3 = npr.sample(size=sample_size)\n        a = [0, 25, 50, 75, 100]\n        rn4 = npr.choice(a, size=sample_size)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "??? shows the results graphically for two continuous distributions and two discrete ones."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [7]: fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))\n        ax1.hist(rn1, bins=25, stacked=True)\n        ax1.set_title('rand')\n        ax1.set_ylabel('frequency')\n        ax1.grid(True)\n        ax2.hist(rn2, bins=25)\n        ax2.set_title('randint')\n        ax2.grid(True)\n        ax3.hist(rn3, bins=25)\n        ax3.set_title('sample')\n        ax3.set_ylabel('frequency')\n        ax3.grid(True)\n        ax4.hist(rn4, bins=25)\n        ax4.set_title('choice')\n        ax4.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"rand_samples\"><img src=\"files/images/rand_samples.png\" alt=\"rand samples\"><figcaption>Simple pseudo-random numbers</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Table 5-2 lists functions for generating (pseudo-) random numbers according to different _distributions_. [11](ch05.html#idp17025168)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table id=\"distributions\">\n<caption>(Pseudo-) Random Numbers from Different Distributions</caption>\n<thead><tr>\n<th>function</th>\n<th>parameters</th>\n<th>description</th>\n</tr></thead>\n<tbody>\n<tr>\n<td><p><code>beta</code></p></td>\n<td><p>(a, b[, size]</p></td>\n<td><p>samples for Beta distribution over [0, 1]</p></td>\n</tr>\n<tr>\n<td><p><code>binomial</code></p></td>\n<td><p>(n, p[, size]</p></td>\n<td><p>samples from a binomial distribution</p></td>\n</tr>\n<tr>\n<td><p><code>chisquare</code></p></td>\n<td><p>(df[, size]</p></td>\n<td><p>samples from a chi-square distribution</p></td>\n</tr>\n<tr>\n<td><p><code>dirichlet</code></p></td>\n<td><p>alpha[, size]</p></td>\n<td><p>samples from the Dirichlet distribution</p></td>\n</tr>\n<tr>\n<td><p><code>exponential</code></p></td>\n<td><p>[scale, size]</p></td>\n<td><p>samples from Exponential distribution</p></td>\n</tr>\n<tr>\n<td><p><code>f</code></p></td>\n<td><p>dfnum, dfden[, size]</p></td>\n<td><p>samples from a F distribution</p></td>\n</tr>\n<tr>\n<td><p><code>gamma</code></p></td>\n<td><p>shape[, scale, size]</p></td>\n<td><p>samples from a Gamma distribution</p></td>\n</tr>\n<tr>\n<td><p><code>geometric</code></p></td>\n<td><p>p[, size]</p></td>\n<td><p>samples from the geometric distribution</p></td>\n</tr>\n<tr>\n<td><p><code>gumbel</code></p></td>\n<td><p>[loc, scale, size]</p></td>\n<td><p>samples from Gumbel distribution</p></td>\n</tr>\n<tr>\n<td><p><code>hypergeometric</code></p></td>\n<td><p>ngood, nbad, nsample[, size]</p></td>\n<td><p>samples from a Hypergeometric distribution</p></td>\n</tr>\n<tr>\n<td><p><code>laplace</code></p></td>\n<td><p>[loc, scale, size]</p></td>\n<td><p>samples from the Laplace or double exponential distribution</p></td>\n</tr>\n<tr>\n<td><p><code>logistic</code></p></td>\n<td><p>[loc, scale, size]</p></td>\n<td><p>samples from a Logistic distribution</p></td>\n</tr>\n<tr>\n<td><p><code>lognormalv</code></p></td>\n<td><p>[mean, sigma, size]</p></td>\n<td><p>samples from a log-normal distribution</p></td>\n</tr>\n<tr>\n<td><p><code>logseries</code></p></td>\n<td><p>p[, size]</p></td>\n<td><p>samples from a Logarithmic Series distribution</p></td>\n</tr>\n<tr>\n<td><p><code>multinomial</code></p></td>\n<td><p>n, pvals[, size]</p></td>\n<td><p>samples from a multinomial distribution</p></td>\n</tr>\n<tr>\n<td><p><code>multivariate_normal</code></p></td>\n<td><p>mean, cov[, size]</p></td>\n<td><p>samples from a multivariate normal distribution</p></td>\n</tr>\n<tr>\n<td><p><code>negative_binomial</code></p></td>\n<td><p>n, p[, size]</p></td>\n<td><p>samples from a negative_binomial distribution</p></td>\n</tr>\n<tr>\n<td><p><code>noncentral_chisquare</code></p></td>\n<td><p>df, nonc[, size]</p></td>\n<td><p>samples from a noncentral chi-square distribution</p></td>\n</tr>\n<tr>\n<td><p><code>noncentral_f</code></p></td>\n<td><p>dfnum, dfden, nonc[, size]</p></td>\n<td><p>samples from the noncentral F distribution</p></td>\n</tr>\n<tr>\n<td><p><code>normal</code></p></td>\n<td><p>[loc, scale, size]</p></td>\n<td><p>samples from a normal (Gaussian) distribution</p></td>\n</tr>\n<tr>\n<td><p><code>pareto</code></p></td>\n<td><p>a[, size]</p></td>\n<td><p>samples from a Pareto II or Lomax distribution with specified shape</p></td>\n</tr>\n<tr>\n<td><p><code>poisson</code></p></td>\n<td><p>[lam, size]</p></td>\n<td><p>samples from a Poisson distribution</p></td>\n</tr>\n<tr>\n<td><p><code>power</code></p></td>\n<td><p>a[, size]</p></td>\n<td><p>samples in [0, 1] from a power distribution with positive exponent a-1</p></td>\n</tr>\n<tr>\n<td><p><code>rayleigh</code></p></td>\n<td><p>[scale, size]</p></td>\n<td><p>samples from a Rayleigh distribution.</p></td>\n</tr>\n<tr>\n<td><p><code>standard_cauchy</code></p></td>\n<td><p>[size]</p></td>\n<td><p>samples from standard Cauchy distribution with mode = 0</p></td>\n</tr>\n<tr>\n<td><p><code>standard_exponential</code></p></td>\n<td><p>[size]</p></td>\n<td><p>samples from the standard exponential distribution</p></td>\n</tr>\n<tr>\n<td><p><code>standard_gamma</code></p></td>\n<td><p>shape[, size]</p></td>\n<td><p>samples from a Standard Gamma distribution</p></td>\n</tr>\n<tr>\n<td><p><code>standard_normal</code></p></td>\n<td><p>[size]</p></td>\n<td><p>samples from a Standard Normal distribution (mean=0, stdev=1)</p></td>\n</tr>\n<tr>\n<td><p><code>standard_t</code></p></td>\n<td><p>df[, size]</p></td>\n<td><p>samples from Student’s t distribution with df degrees of freedom</p></td>\n</tr>\n<tr>\n<td><p><code>triangular</code></p></td>\n<td><p>left, mode, right[, size]</p></td>\n<td><p>samples from the triangular distribution</p></td>\n</tr>\n<tr>\n<td><p><code>uniform</code></p></td>\n<td><p>[low, high, size]</p></td>\n<td><p>samples from a uniform distribution</p></td>\n</tr>\n<tr>\n<td><p><code>vonmises</code></p></td>\n<td><p>mu, kappa[, size]</p></td>\n<td><p>samples from a von Mises distribution</p></td>\n</tr>\n<tr>\n<td><p><code>wald</code></p></td>\n<td><p>mean, scale[, size]</p></td>\n<td><p>samples from a Wald, or Inverse Gaussian, distribution</p></td>\n</tr>\n<tr>\n<td><p><code>weibull</code></p></td>\n<td><p>a[, size]</p></td>\n<td><p>samples from Weibull distribution</p></td>\n</tr>\n<tr>\n<td><p><code>zipf</code></p></td>\n<td><p>a[, size]</p></td>\n<td><p>samples from a Zipf distribution</p></td>\n</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Although there is much criticism around the use of (standard) normal distributions in finance, they are an indispensible tool and still the most widely used type of distribution. One reason is that many financial models directly rest in one way or another of a normal distribution or a log-normal distribution. Another one is that many financial models that do not rest directly on (log-) normal assumption can be discretized, and therewith approximated for simulation purposes, by the use of the normal distribution."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As illustration, we want to visualize random draws from the following distributions:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p><strong>standard normal</strong> with mean of zero and standard deviation of one</p>\n</li>\n<li>\n<p><strong>normal</strong> with mean of 100 and standard deviation of 20</p>\n</li>\n<li>\n<p><strong>chi square</strong> with 0.5 degrees of freedom</p>\n</li>\n<li>\n<p><strong>Poisson</strong> with lambda of 1</p>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [8]: sample_size = 500\n        rn1 = npr.standard_normal(sample_size)\n        rn2 = npr.normal(100, 20, sample_size)\n        rn3 = npr.chisquare(df=0.5, size=sample_size)\n        rn4 = npr.poisson(lam=1.0, size=sample_size)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-2 shows the results for the three continuous distributions and the discrete one (Poisson). The Poisson distribution is used, for example, to simulate the arrival or (rare) externel events, like a jump in the price of an instrument or an exogenic shock."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [9]: fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))\n        ax1.hist(rn1, bins=25)\n        ax1.set_title('standard normal')\n        ax1.set_ylabel('frequency')\n        ax1.grid(True)\n        ax2.hist(rn2, bins=25)\n        ax2.set_title('normal(100, 20)')\n        ax2.grid(True)\n        ax3.hist(rn3, bins=25)\n        ax3.set_title('chi square')\n        ax3.set_ylabel('frequency')\n        ax3.grid(True)\n        ax4.hist(rn4, bins=25)\n        ax4.set_title('Poisson')\n        ax4.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"rand_distris\"><img src=\"files/images/rand_distris.png\" alt=\"rand distris\"><figcaption>Pseudo-random numbers from different distributions</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Simulation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Monte Carlo simulation (MCS) is among the most important numerical techniques in finance, if not _the_ most important and widely used. This mainly stems from the fact that it is most flexible when it comes to the evaluation of mathematical expressions (e.g. integrals) or specifically the valuation of financial derivatives. The flexibility comes at the cost of a relatively high computational burden since often 100,000s or 1,000,000s of complex computations have to be carried out to come up with a single value estimate only."
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "Random Variables"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider, for example, the Black-Scholes-Merton set-up for option pricing. In their set-up, the level of a stock index $S\\_T$ at a future date $T$ given a level $S\\_0$ as of today is given according to ???."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The variables and parameters have the following meaning:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p><span data-type=\"tex\">$S_t$</span> index level at date <span data-type=\"tex\">$t$</span></p>\n</li>\n<li>\n<p><span data-type=\"tex\">$r$</span> constant risk-less short rate</p>\n</li>\n<li>\n<p><span data-type=\"tex\">$\\sigma$</span> constant volatility (= standard deviation of returns) of <span data-type=\"tex\">$S$</span></p>\n</li>\n<li>\n<p><span data-type=\"tex\">$z$</span> standard normally distributed random variable</p>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This simple financial model is easily parameterized and simulated as follows."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [10]: S0 = 100  # initial value\n         r = 0.05  # constant short rate\n         sigma = 0.25  # constant volatility\n         T = 2.0  # in years\n         I = 10000  # number of random draws\n         ST1 = S0 * np.exp((r - 0.5 * sigma ** 2) * T\n                      + sigma * np.sqrt(T) * npr.standard_normal(I))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The output of this simulation code is shown in Figure 5-3."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [11]: plt.hist(ST1, bins=50)\n         plt.xlabel('index level')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"gbm_T_sn\"><img src=\"files/images/gbm_T_sn.png\" alt=\"gbm T sn\"><figcaption>Simulated geometric Brownian motion (via <code>standard_normal</code>)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-3 suggests, that the distribution of the random variable as defined in Simulating future index level in Black-Scholes-Merton set-up is _log-normal_. We could therefore also try to use the `lognormal` function to directly derive the values for the random variable. In that case, we have to provide the mean and the standard deviation to the respective function."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [12]: ST2 = S0 * npr.lognormal((r - 0.5 * sigma ** 2) * T,\n                                 sigma * np.sqrt(T), size=I)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By visual inspection, Figure 5-4 and Figure 5-3 look indeed pretty similar. But let us verify this more rigorously by comparing statistical moments of the resulting distributions."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [13]: plt.hist(ST2, bins=50)\n         plt.xlabel('index level')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"gbm_T_ln\"><img src=\"files/images/gbm_T_ln.png\" alt=\"gbm T ln\"><figcaption>Simulated geometrix Brownian motion (via <code>lognormal</code>)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To compare the distributional characteristics of simulation results we use here and in the following the the `scipy.stats` sub-library and the helper function `print_statistics` as defined below."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [14]: import scipy.stats as scs",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [15]: def print_statistics(a1, a2):\n             ''' Prints selected statistics.\n\n             Parameters\n             ==========\n             a1, a2 : ndarray objects\n                 results object from simulation\n             '''\n             sta1 = scs.describe(a1)\n             sta2 = scs.describe(a2)\n             print \"%14s %14s %14s\" % \\\n                 ('statistic', 'data set 1', 'data set 2')\n             print 45 * \"-\"\n             print \"%14s %14.3f %14.3f\" % ('size', sta1[0], sta2[0])\n             print \"%14s %14.3f %14.3f\" % ('min', sta1[1][0], sta2[1][0])\n             print \"%14s %14.3f %14.3f\" % ('max', sta1[1][1], sta2[1][1])\n             print \"%14s %14.3f %14.3f\" % ('mean', sta1[2], sta2[2])\n             print \"%14s %14.3f %14.3f\" % ('std', np.sqrt(sta1[3]), np.sqrt(sta2[3]))\n             print \"%14s %14.3f %14.3f\" % ('skew', sta1[4], sta2[4])\n             print \"%14s %14.3f %14.3f\" % ('kurtosis', sta1[5], sta2[5])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [16]: print_statistics(ST1, ST2)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[16]:      statistic     data set 1     data set 2\n         ---------------------------------------------\n                   size      10000.000      10000.000\n                    min         29.735         27.953\n                    max        471.806        408.615\n                   mean        110.380        110.539\n                    std         40.322         40.664\n                   skew          1.225          1.230\n               kurtosis          2.998          2.853",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Obviously, the statistics of both simulation results are quite similar. Differences are mainly due to what is called the _sampling error_ in simulation. Another source of error is introduced when simulating stochastic processes, namely the _discretization error_ which play no role here due to the static nature of the simulation approach."
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "Stochastic Processes"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Roughly speaking, a _stochastic process_ is a sequence of random variables. In that sense, we should expect something similar to a squence of repeated simulations of a random variable when simulating a process. This is mainly true apart from the fact that the draws are in general not independent but rather depend on the result(s) of the previous draw(s)."
        },
        {
          "cell_type": "heading",
          "level": 4,
          "metadata": {
          },
          "source": "Geometric Brownian Motion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider now the Black-Scholes-Merton model in its dynamic form as described by the stochastic differential equation (SDE) in Stochastic differential equation in Black-Scholes-Merton set-up. Here, $Z\\_t$ is a standard Brownian motion. The SDE is called a _geometric Brownian motion_. The values of $S\\_t$ are log-normally distributed and the returns $\\frac{dS\\_t}{S\\_t}$ normally."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The SDE in Stochastic differential equation in Black-Scholes-Merton set-up can be discretized exactly by an Euler scheme. Such a scheme is presented in Simulating index levels dynamically in Black-Scholes-Merton set-up with $\\Delta t$ being the fixed discretization interval and $z\\_t$ being a standard normally distributed random variable."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As before, translation into `Python` and `NumPy` code is straightforward."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [17]: I = 10000\n         M = 50\n         dt = T / M\n         S = np.zeros((M + 1, I))\n         S[0] = S0\n         for t in range(1, M + 1):\n             S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt\n                     + sigma * np.sqrt(dt) * npr.standard_normal(I))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The resulting end values for the index level are log-normally distributed again as Figure 5-5 illustrates."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [18]: plt.hist(S[-1], bins=50)\n         plt.xlabel('index level')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"gbm_dt_hist\"><img src=\"files/images/gbm_dt_hist.png\" alt=\"gbm dt hist\"><figcaption>Simulated geometric Brownian motion at maturity</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This first four moments are also quite close to those resulting from the staic approach."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [19]: print_statistics(S[-1], ST2)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[19]:      statistic     data set 1     data set 2\n         ---------------------------------------------\n                   size      10000.000      10000.000\n                    min         24.368         27.953\n                    max        357.488        408.615\n                   mean        109.708        110.539\n                    std         40.590         40.664\n                   skew          1.248          1.230\n               kurtosis          2.717          2.853",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using the dynamic simulation approach not only allows to visualize paths as displayed in Figure 5-6 but also to value options with American/Bermudan exercise or path-dependent options. You get the full dynamic picture, so to say."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [20]: plt.plot(S[:, :10], lw=1.5)\n         plt.xlabel('time')\n         plt.ylabel('index level')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"gbm_dt_paths\"><img src=\"files/images/gbm_dt_paths.png\" alt=\"gbm dt paths\"><figcaption>Simulated geometric Brownian motion paths</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 4,
          "metadata": {
          },
          "source": "Square-Root Diffusion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Another important class of financial processes are mean-reverting processes which are used to model short rates or volatility processes, for example. A popular and widely used model is the _square-root diffusion_ as proposed by Cox-Ingersoll-Ross (1985). Stochastic differential equation for square-root diffusion provides the respective SDE."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "It is well known that the values of $x\\_t$ are chi-squared distributed. However, as stated before many financial models can be discretized and approximated by using the normal distributed, i.e. a so-called Euler discretization scheme. While the Euler scheme is exact for the geometric Browian motion, it is biased for the majority of other stochastics processes. Even if there is an exact scheme available&ndash;one for the square-root diffusion will be presented shortly&ndash;the use of an Euler schememight be desirable due to numerical and/or computational reasons. Definin $s \\equiv t - \\Delta t$, Euler discretization for square-root diffusion presents such an Euler scheme. This particular one is generally called _full truncation_ in the literature."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We parameterize the model for the simulations to follows with values that could represent those of a short rate model."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [21]: x0 = 0.05\n         kappa = 3.0\n         theta = 0.02\n         sigma = 0.1",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The square-root diffusion has the convenient and realistic characteristic that the values of $x\\_t$ remain strictly positive. When discretizing it by an Euler scheme, negative values cannot be excluded. That is the reason why one works always with the positive version of the originally simulated process. In the simulation code, one therefore needs two `ndarray` objects instead of only one."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [22]: I = 10000\n         M = 50\n         dt = T / M\n         def srd_euler():\n             xh = np.zeros((M + 1, I))\n             x1 = np.zeros_like(xh)\n             xh[0] = x0\n             x1[0] = x0\n             for t in range(1, M + 1):\n                 xh[t] = (xh[t - 1] + kappa * (theta - np.maximum(xh[t - 1], 0)) * dt\n                       + sigma * np.sqrt(np.maximum(xh[t - 1], 0)) * np.sqrt(dt)\n                       * npr.standard_normal(I))\n                 x1[t] = np.maximum(xh[t], 0)\n             return x1\n         x1 = srd_euler()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-7 shows the result of the simulation graphically as a histogram."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [23]: plt.hist(x1[-1], bins=50)\n         plt.xlabel('value')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"srd_hist_Euler\"><img src=\"files/images/srd_hist_Euler.png\" alt=\"srd hist Euler\"><figcaption>Simulated square-root diffusion at maturity (Euler scheme)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "??? the first ten simulated paths, illustrating the resulting negative drift (due to $x\\_0>\\theta$) and the convergence to $\\theta=0.02$."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [24]: plt.plot(x1[:, :10], lw=1.5)\n         plt.xlabel('time')\n         plt.ylabel('index level')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"srd_dt_Euler\"><img src=\"files/images/srd_dt_Euler.png\" alt=\"srd dt Euler\"><figcaption>Simulated square-root diffusion paths (Euler scheme)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let us now get more exact. Exact discretization for square-root diffusion presents the exact discretization scheme for the square-root diffusion based on the non-central chi square distribution $\\chi^{’2}\\_{d}$ with $df=\\frac{4\\theta\\kappa}{\\sigma^{2}}$ degrees of freedom and non-centrality parameter $nc=\\frac{4\\kappa e^{-\\kappa \\Delta t}}{\\sigma^{2}(1-e^{-\\kappa\\Delta t})}x\\_{s}$."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The `Python` implementation of this discretization scheme is a bit more involved but still quite concise."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [25]: def srd_exact():\n             x2 = np.zeros((M + 1, I))\n             x2[0] = x0\n             for t in range(1, M + 1):\n                 df = 4 * theta * kappa / sigma ** 2\n                 c = (sigma ** 2 * (1 - np.exp(-kappa * dt))) / (4 * kappa)\n                 nc = np.exp(-kappa * dt) / c * x2[t - 1]\n                 x2[t] = c * npr.noncentral_chisquare(df, nc, size=I)\n             return x2\n         x2 = srd_exact()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-9 shows the output of the simulation with the exact scheme as a histogram."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [26]: plt.hist(x2[-1], bins=50)\n         plt.xlabel('value')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"srd_hist_exact\"><img src=\"files/images/srd_hist_exact.png\" alt=\"srd hist exact\"><figcaption>Simulated square-root diffusion at maturity (exact scheme)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "??? presents as before the first ten simulated paths, again displaying the negative average drift and the convergence to $\\theta$."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [27]: plt.plot(x2[:, :10], lw=1.5)\n         plt.xlabel('time')\n         plt.ylabel('index level')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"srd_dt_exact\"><img src=\"files/images/srd_dt_exact.png\" alt=\"srd dt exact\"><figcaption>Simulated square-root diffusion paths (exact scheme)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Comparing the main statistics from the different approaches reveals that the biased Euler scheme indeed performs quite well when it comes to the desired statistical properties."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [28]: print_statistics(x1[-1], x2[-1])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[28]:      statistic     data set 1     data set 2\n         ---------------------------------------------\n                   size      10000.000      10000.000\n                    min          0.005          0.006\n                    max          0.055          0.049\n                   mean          0.020          0.020\n                    std          0.006          0.006\n                   skew          0.552          0.579\n               kurtosis          0.496          0.452",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "However, a major difference can be observed in terms of execution speed since sampling from the non-central chi square distribution is more computationally demanding than from the standard normal distribution. To illustrate this point, consider a higher number of paths to be simulated."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [29]: I = 250000\n         %time x1 = srd_euler()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[29]: CPU times: user 975 ms, sys: 224 ms, total: 1.2 s\n         Wall time: 1.2 s",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [30]: %time x2 = srd_exact()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[30]: CPU times: user 1.99 s, sys: 48.3 ms, total: 2.04 s\n         Wall time: 2.04 s",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The exact scheme takes roughly twice as much time for virutally the same results as with the Euler scheme."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [31]: print_statistics(x1[-1], x2[-1])\n         x1 = 0.0; x2 = 0.0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[31]:      statistic     data set 1     data set 2\n         ---------------------------------------------\n                   size     250000.000     250000.000\n                    min          0.001          0.003\n                    max          0.063          0.057\n                   mean          0.020          0.020\n                    std          0.006          0.006\n                   skew          0.560          0.576\n               kurtosis          0.477          0.486",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "heading",
          "level": 4,
          "metadata": {
          },
          "source": "Stochastic Volatility"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "One of the major simplifying assumptions of the Black-Scholes-Merton model is the _constant_ volatility. Not only that volatility in general is neither constant nor deterministic, it is _stochastic_. Therefore, a major advancements with regard to financial modeling has been achieved mainly beginning in the early 1990s with the introduction of so-called _stochastic voltility models_. One of the most popular models that fall into that category is the one of Heston (1993) which is presented in Stochastic differential equations for Heston stochastic volatility model."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The meaning of the single variables and parameters can now be inferred easily from the discussion of the geometric Brownian motion and the square-root diffusion. The parameter $\\rho$ represents the instantaneous correlation between the two standard Brownian motions $Z^1\\_t, Z^2\\_t$. This allows to account for the stylized fact called _leverage effect_, which in essence states that volatility goes up in times of stress (declining markets) and goes down in times of a bull market (rising markets)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider the following parametrization of the model."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [32]: S0 = 100.\n         r = 0.05\n         v0 = 0.1\n         kappa = 3.0\n         theta = 0.25\n         sigma = 0.1\n         rho = 0.6",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To account for the correlation between the two stochastic processes, we need to determine the cholesky decomposition of the correlation matrix."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [33]: corr_mat = np.zeros((2, 2))\n         corr_mat[0, :] = [1.0, rho]\n         corr_mat[1, :] = [rho, 1.0]\n         cho_mat = np.linalg.cholesky(corr_mat)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [34]: cho_mat",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[34]: array([[ 1. ,  0. ],\n                [ 0.6,  0.8]])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Before we start simulating the stochastic processes, we generate the whole set of random numbers for both processes, looking to use set 0 for the index process and set 1 for the volatility process."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [35]: ran_num = npr.standard_normal((2, M + 1, 10000))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For the volatility process, we use the Euler scheme for the square-root diffusion, taking into account the correlation."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [36]: M = 50\n         dt = T / M\n         v = np.zeros_like(ran_num[0])\n         vh = np.zeros_like(v)\n         v[0] = v0\n         vh[0] = v0\n         for t in range(1, M + 1):\n             ran = np.dot(cho_mat, ran_num[:, t, :])\n             vh[t] = (vh[t - 1] + kappa * (theta - np.maximum(vh[t - 1], 0)) * dt\n                   + sigma * np.sqrt(np.maximum(vh[t - 1], 0)) * np.sqrt(dt)\n                   * ran[1])\n             v[t] = np.maximum(vh[t], 0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For the index level process, we also take into account the correlation and use the exact Euler scheme for the geometric Brownian motion."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [37]: S = np.zeros_like(ran_num[0])\n         S[0] = S0\n         for t in range(1, M + 1):\n             ran = np.dot(cho_mat, ran_num[:, t, :])\n             S[t] = S[t - 1] * exp((r - 0.5 * v[t]) * dt +\n                             sqrt(v[t]) * ran[0] * sqrt(dt))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This illustrates another advantage of working with the Euler scheme for the square-root diffusion: _correlation is easily and consistently accounted for_ since we only draw standard normally distributed random numbers. There is no simple way of achieving the same with a mixed approach, using Euler for the index and the non-central chi square-based exact approach for the volatility process."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-11 shows the simulation results as histogram for both the index level process and the volatility process."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [38]: fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n         ax1.hist(S[-1], bins=50)\n         ax1.set_xlabel('index level')\n         ax1.set_ylabel('frequency')\n         ax1.grid(True)\n         ax2.hist(v[-1], bins=50)\n         ax2.set_xlabel('volatility')\n         ax2.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"sv_hist\"><img src=\"files/images/sv_hist.png\" alt=\"sv hist\"><figcaption>Simulated stochastic volatility model at maturity</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "An inspection of the first 10 simulated paths of each process no shows that the volatility process is drifting positively on average and that it, as expected, converges to $\\theta\\_v=0.25$."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [39]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(7, 6))\n         ax1.plot(S[:, :10], lw=1.5)\n         ax1.set_ylabel('index level')\n         ax1.grid(True)\n         ax2.plot(v[:, :10], lw=1.5)\n         ax2.set_xlabel('time')\n         ax2.set_ylabel('volatility')\n         ax2.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"sv_paths\"><img src=\"files/images/sv_paths.png\" alt=\"sv paths\"><figcaption>Simulated stochastic volatility model paths</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally, a brief look at the statistics for the last point in time for both data sets, showing a pretty high maximum value for the index level process. In fact, much higher than a geometric Brownian motion with constant volatility could ever climb ceteris paribus."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [40]: print_statistics(S[-1], v[-1])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[40]:      statistic     data set 1     data set 2\n         ---------------------------------------------\n                   size      10000.000      10000.000\n                    min          8.934          0.179\n                    max       1603.078          0.343\n                   mean        116.872          0.250\n                    std         89.395          0.021\n                   skew          3.045          0.195\n               kurtosis         19.968          0.047",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "heading",
          "level": 4,
          "metadata": {
          },
          "source": "Jump-Diffusion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Stochastic volatility and the leverage effect are stylized (empirical) facts found in a number of markets. Another important stylized empirical fact is the existence of _jumps_ in asset prices and, for example, volatility. Already in 1976 Merton published his jump-diffusion model enhancing the Black-Scholes-Merton set-up by a jump component genering jumps with log-normal distribution. The SDE is presented in Stochastic differential equation for Merton jump diffusion model."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For completeness, an overview of the variables’ and parameters’ meaning:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p><span data-type=\"tex\">$S_{t}$</span> index level at date <span data-type=\"tex\">$t$</span></p>\n</li>\n<li>\n<p><span data-type=\"tex\">$r$</span> constant risk-less short rate</p>\n</li>\n<li>\n<p><span data-type=\"tex\">$r_{J}\\equiv \\lambda \\cdot \\left(e^{\\mu_{J}+\\delta^{2}/2}-1\\right)$</span> drift correction for jump to maintain risk-neurtrality</p>\n</li>\n<li>\n<p><span data-type=\"tex\">$\\sigma$</span> constant volatility of <span data-type=\"tex\">$S$</span></p>\n</li>\n<li>\n<p><span data-type=\"tex\">$Z_{t}$</span> standard Brownian motion</p>\n</li>\n<li>\n<p><span data-type=\"tex\">$J_{t}$</span> jump at date <span data-type=\"tex\">$t$</span> with distribution …</p>\n</li>\n<li>\n<p>… <span data-type=\"tex\">$\\log(1+J_{t})\\approx \\mathbf{N}\\left(\\log(1+\\mu_{J})-\\frac{\\delta^{2}}{2},\\delta^{2}\\right)$</span> with …</p>\n</li>\n<li>\n<p>… <span data-type=\"tex\">$\\mathbf{N}$</span> as the cumulative distribution function of a standard normal random variable</p>\n</li>\n<li>\n<p><span data-type=\"tex\">$N_{t}$</span> Poisson process with intensity <span data-type=\"tex\">$\\lambda$</span></p>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Euler discretization for Merton jump diffusion model presents an Euler discretization for the jump-diffusion where the $z^{n}\\_{t}$ are standard normally distributed and the $y\\_{t}$ are Poisson distributed with intensity $\\lambda$."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Given the discretization scheme, consider the following numerical parametrization."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [41]: S0 = 100.\n         r = 0.05\n         sigma = 0.2\n         lamb = 0.75\n         mu = -0.6\n         delta = 0.25",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To simulate the jump-diffusion, we need to generate three sets of (independent) random numbers."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [42]: I = 10000\n         dt = T / M\n         rj = lamb * (np.exp(mu + 0.5 * delta ** 2) - 1)\n         S = np.zeros((M + 1, I))\n         S[0] = S0\n         sn1 = npr.standard_normal((M + 1, I))\n         sn2 = npr.standard_normal((M + 1, I))\n         poi = npr.poisson(lamb * dt, (M + 1, I))\n         for t in range(1, M + 1, 1):\n             S[t] = S[t - 1] * (np.exp((r - rj - 0.5 * sigma ** 2) * dt\n                                + sigma * np.sqrt(dt) * sn1[t])\n                                + (np.exp(mu + delta * sn2[t]) - 1)\n                                * poi[t])\n             S[t] = np.maximum(S[t], 0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Since we have assumed a highly negative mean for the jump, it should not come as a surprise that the final values of the simulated index level are much more _left skewed_ in Figure 5-13 compared to a typical log-normal distribution."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [43]: plt.hist(S[-1], bins=50)\n         plt.xlabel('value')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"jd_hist\"><img src=\"files/images/jd_hist.png\" alt=\"jd hist\"><figcaption>Simulated jump diffusion at maturity</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The highly negative jumps can also be found in the first 10 simulated index level paths as presented in Figure 5-14."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [44]: plt.plot(S[:, :10], lw=1.5)\n         plt.xlabel('time')\n         plt.ylabel('index level')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"jd_paths\"><img src=\"files/images/jd_paths.png\" alt=\"jd paths\"><figcaption>Simulated jump diffusion paths</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "Variance Reduction"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Not only because of the fact that the `Python` functions we have used so far generate _pseudo-random_ numbers but also due to varying sizes of the samples draws, resulting sets of numbers might not exhibit statistics really close enough to the expected/desired ones. For example, you would expect a set of standard normally distributed random numbers to show a mean of **0** and a standard deviation of **1**. Let us check what statistics different sets of random numbers exhibit. To achieve a realistic comparison, we fix the seed value for the random number generator."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [45]: print \"%15s %15s\" % ('Mean', 'Std. Deviation')\n         print 31 * \"-\"\n         for i in range(1, 31, 2):\n             npr.seed(1000)\n             sn = npr.standard_normal(i ** 2 * 10000)\n             print \"%15.12f %15.12f\" % (sn.mean(), sn.std())",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[45]:            Mean  Std. Deviation\n         -------------------------------\n         -0.011870394558  1.008752430725\n         -0.002815667298  1.002729536352\n         -0.003847776704  1.000594044165\n         -0.003058113374  1.001086345326\nOut[45]: -0.001685126538  1.001630849589\nOut[45]: -0.001175212007  1.001347684642\nOut[45]: -0.000803969036  1.000159081432\nOut[45]: -0.000601970954  0.999506522127\nOut[45]: -0.000147787693  0.999571756099\nOut[45]: -0.000313035581  0.999646153705\nOut[45]: -0.000178447061  0.999677277878\nOut[45]:  0.000096501709  0.999684346792\nOut[45]: -0.000135677013  0.999823841902\nOut[45]: -0.000015726986  0.999906493379\nOut[45]: -0.000039368519  1.000063091949",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [46]: i ** 2 * 10000",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[46]: 8410000",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The results show that the statistics “somehow” get better the larger the number of draws become. But they still do not match the desired one, even in our largest sample with more than 8,000,000 random numbers."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Fortunately, there easy to implement, generic _variance reduction_ techniques available to improve the matching of the first two moments of the (standard) normal distribution. The first are _antithetic variates_. This approach simply draws only half the number of the desired random draws and adds the same set of random numbers with the opposite afterwards. For example, if the random number generator (i.e. the respective `Python` function) draws a **0.5** then another number with value **-0.5** is added to the set."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "With `NumPy` this is concisely implemented by using the function `concatenate` ."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [47]: sn = npr.standard_normal(10000 / 2)\n         sn = np.concatenate((sn, -sn))\n         np.shape(sn)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[47]: (10000,)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The following repeats the exercise from before, this time using antithetic variates."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [48]: print \"%15s %15s\" % ('Mean', 'Std. Deviation')\n         print 31 * \"-\"\n         for i in range(1, 31, 2):\n             npr.seed(1000)\n             sn = npr.standard_normal(i ** 2 * 10000 / 2)\n             sn = np.concatenate((sn, -sn))\n             print \"%15.12f %15.12f\" % (sn.mean(), sn.std())",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[48]:            Mean  Std. Deviation\n         -------------------------------\n          0.000000000000  1.009653753942\n          0.000000000000  1.000413716783\n         -0.000000000000  1.002925061201\n          0.000000000000  1.000755212673\nOut[48]:  0.000000000000  1.001636910076\n          0.000000000000  1.000726758438\nOut[48]: -0.000000000000  1.001621265149\nOut[48]:  0.000000000000  1.001203722778\nOut[48]:  0.000000000000  1.000556669785\nOut[48]:  0.000000000000  1.000113464185\nOut[48]:  0.000000000000  0.999435175324\nOut[48]: -0.000000000000  0.999356961431\nOut[48]:  0.000000000000  0.999641436845\nOut[48]: -0.000000000000  0.999642768905\nOut[48]:  0.000000000000  0.999638303451",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As you immediately notice, this approach corrects the first moment perfectly which should not come as a surprise. This follows from the fact that whenever a number $n$ is drawn, $-n$ is also added. Since we only have such pairs, the mean is equal to 0 over the whole set of random numbers. However, this approach does not have any influence on the second moment, the standard deviation."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using another variance reduction technique called _moment matching_ helps correcting in one step both the first and second moments."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [49]: sn = npr.standard_normal(10000)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [50]: sn.mean()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[50]: -0.0011659982951625055",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [51]: sn.std()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[51]: 0.99125592020460507",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By subtracting the mean from every single random number and dividing every single number by the standard deviation we get a set of random numbers matching the desired first and second moments of the standard normal distribution (almost) perfectly)."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [52]: sn_new = (sn - sn.mean()) / sn.std()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [53]: sn_new.mean()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[53]: -2.5135449277513543e-17",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [54]: sn_new.std()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[54]: 0.99999999999999889",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The following function utilizes the insight with regard to variance reduction techniques and generates standard normally random numbers for process simulation using either two, one or no variance reduction technique."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [55]: def gen_sn(M, I, anti_paths=True, mo_match=True):\n             ''' Function to generate random numbers for simulation.\n\n             Parameters\n             ==========\n             M : int\n                 number of time intervals for discretization\n             I : int\n                 number of paths to be simulated\n             anti_paths: boolean\n                 use of antithetic variates\n             mo_math : booean\n                 use of moment matching\n             '''\n             if anti_paths is True:\n                 sn = npr.standard_normal((M + 1, I / 2))\n                 sn = np.concatenate((sn, -sn), axis=1)\n             else:\n                 sn = npr.standard_normal((M + 1, I))\n             if mo_match is True:\n                 sn = (sn - sn.mean()) / sn.std()\n             return sn",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Valuation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "One of the most important applications of Moonte Carlo simulation is the _valuation of contingent claims_ (options, derivatives, hybrid instruments, etc.). Simply spoken, in a risk-neutral world the value of a contingent claim is the discounted expected payoff under the risk-neutral (martingale) measure. This is the probability measure that makes all risk factors (stock, indexes, etc.) drift at the risk-less short rate. According to the Fundamental Theorem of Asset Pricing, the existence of such a probability measure is equivalent to the absence of arbitrage."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A financial option embodies the right to buy (_call option_) or sell (_put option_) a specified financial instrument at a given (maturity) date (_European option_), or over a specified period of time (_American option_) at a given price, the so-called strike price. Let us first consider the much simpler case of European options in terms of valuation."
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "European Options"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The payoff of a European call option on an index at maturity is given by $h(S\\_T) \\equiv \\max[S\\_T - K, 0]$ where $S\\_T$ is the index level at maturity date $T$ and $K$ is the strike price. Given a, or in complete markets _the_, risk-neutral measure for the relevant stochastic process (e.g. geometric Brownian motion), the price of such an option is given by the formula in Pricing by risk-neutral expectation."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "[Chapter 4](ch04.html#math_tools) briefly sketches how to numerically evaluation an integral by Monte Carlo simulation. This approach is used in the following and applied to Pricing by risk-neutral expectation. Risk-neutral Monte Carlo estimator provides the respective Monte Carlo estimator for the European option."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider now the following parametrization for the geometric Brownian motion and the valuation function `gbm_mcs_stat` taking as parameter only the strike price. Here, only the index level at maturity is simulated."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [56]: S0 = 100.\n         r = 0.05\n         sigma = 0.25\n         T = 1.0\n         I = 50000\n         def gbm_mcs_stat(K):\n             ''' Valuation of European call option in Black-Scholes-Merton\n             by Monte Carlo simulation (of index level at maturity)\n\n             Parameters\n             ==========\n             K : float\n                 (positive) strike price of the option\n\n             Returns\n             =======\n             C0 : float\n                 estimated present value of European call option\n             '''\n             sn = gen_sn(1, I)\n             # simulate index level at maturity\n             ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T\n                          + sigma * np.sqrt(T) * sn[1])\n             # calculate payoff at maturity\n             hT = np.maximum(ST - K, 0)\n             # calculate MCS estimator\n             C0 = np.exp(-r * T) * 1 / I * np.sum(hT)\n             return C0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As reference, consider the case with a strike price of $K=105$."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [57]: gbm_mcs_stat(K=105.)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[57]: 10.044221852841966",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Next, we consider the dynamic simulation approach and allow also for European put options in addition to the call option. Function `gbm_mcs_dyna` implements the algorithm."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [58]: M = 50\n         def gbm_mcs_dyna(K, option='call'):\n             ''' Valuation of European options in Black-Scholes-Merton\n             by Monte Carlo simulation (of index level paths)\n\n             Parameters\n             ==========\n             K : float\n                 (positive) strike price of the option\n             option : string\n                 type of the option to be valued ('call', 'put')\n\n             Returns\n             =======\n             C0 : float\n                 estimated present value of European call option\n             '''\n             dt = T / M\n             # simulation of index level paths\n             S = np.zeros((M + 1, I))\n             S[0] = S0\n             sn = gen_sn(M, I)\n             for t in range(1, M + 1):\n                 S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt\n                         + sigma * np.sqrt(dt) * sn[t])\n             # case-based calculation of payoff\n             if option == 'call':\n                 hT = np.maximum(S[-1] - K, 0)\n             else:\n                 hT = np.maximum(K - S[-1], 0)\n             # calculation of MCS estimator\n             C0 = np.exp(-r * T) * 1 / I * np.sum(hT)\n             return C0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Now, we can compare option price estimates for a call and a put stroke at the same level."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [59]: gbm_mcs_dyna(K=110., option='call')",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[59]: 7.9500085250285508",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [60]: gbm_mcs_dyna(K=110., option='put')",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[60]: 12.629934942681798",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The question is how good do these simulation-based valuation approaches perform relative to the benchmark value from the Black-Scholes-Merton valuation formula. To this end, let us generate respective option values/estimates for a range of strike prices, using the analytical option pricing formula for European calls in Black-Scholes-Merton found in module `BSM_Functions.py` ."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [61]: from BSM_Functions import BSM_Call_Value\n         stat_res = []\n         dyna_res = []\n         anal_res = []\n         k_list = np.arange(80., 120.1, 5.)\n         np.random.seed(200000)\n         for K in k_list:\n             stat_res.append(gbm_mcs_stat(K))\n             dyna_res.append(gbm_mcs_dyna(K))\n             anal_res.append(BSM_Call_Value(S0, K, T, r, sigma))\n         stat_res = np.array(stat_res)\n         dyna_res = np.array(dyna_res)\n         anal_res = np.array(anal_res)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "First, we compare the results from the static simulation approach with prescise analytical values. Figure 5-15 shows the results. All valuation differences are smaller than **1%** absolutely. There are both negative and positive value differences."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [62]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))\n         ax1.plot(k_list, anal_res, 'b', label='analytical')\n         ax1.plot(k_list, stat_res, 'ro', label='static')\n         ax1.set_ylabel('European call option value')\n         ax1.grid(True)\n         ax1.legend(loc=0)\n         ax1.set_ylim(ymin=0)\n         wi = 1.0\n         ax2.bar(k_list - wi / 2, (anal_res - stat_res) / anal_res * 100, wi)\n         ax2.set_xlabel('strike')\n         ax2.set_ylabel('difference in %')\n         ax2.set_xlim(left=75, right=125)\n         ax2.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"opt_val_comp_1\"><img src=\"files/images/opt_val_comp_1.png\" alt=\"opt val comp 1\"><figcaption>Comparsion of static and dynamic Monte Carlo estimator values</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A similar picture emerges for the dynamic simulation and valuation approach whose results are reported in Figure 5-16. Again, all valuation differences are smaller than **1%** absolutely with both positive as well as negative deviations. As a general rule, the quality of the Monte Carlo estimator can be controlled for by adjusting the number of time intervals $M$ used and/or the number of paths $I$ simulated."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [63]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))\n         ax1.plot(k_list, anal_res, 'b', label='analytical')\n         ax1.plot(k_list, dyna_res, 'ro', label='dynamic')\n         ax1.set_ylabel('European call option value')\n         ax1.grid(True)\n         ax1.legend(loc=0)\n         ax1.set_ylim(ymin=0)\n         wi = 1.0\n         ax2.bar(k_list - wi / 2, (anal_res - dyna_res) / anal_res * 100, wi)\n         ax2.set_xlabel('strike')\n         ax2.set_ylabel('difference in %')\n         ax2.set_xlim(left=75, right=125)\n         ax2.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"opt_val_comp_2\"><img src=\"files/images/opt_val_comp_2.png\" alt=\"opt val comp 2\"><figcaption>Comparsion of static and dynamic Monte Carlo estimator values</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "American Options"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The valuation of American options is more involved compared to European options. In this case, an _optimal stopping_ problem has to be solved to come up with a fair value of the option. American option prices as optimal stopping problem formulates the valuation of an American option as such an problem. The problem formulation is already based on a discrete time grid for the use with numerical simulation. In a sense, it is therefore more correct to speak of an option value given _Bermudan_ exercise. For the time interval converging to zero, the value of the Bermudan option converges to the one of the American option."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The algorithm we describe in the following is called _Least-Squares Monte Carlo_ and is from the paper by Longstaff-Schwartz (2001). It can be shown that the value of an American (Bermudan) option at any given date $t$ is given as $V\\_{t}(s)=\\max(h\\_{t}(s),C\\_{t}(s))$ where $C\\_{t}(s)=\\textbf{E}\\_{t}^{Q}(e^{-r \\Delta t} V\\_{t+\\Delta t}(S\\_{t+\\Delta t})|S\\_{t}=s)$ is the so-called continuation value of the option given an index level of $S\\_t=s$."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider now that we have simulated $I$ paths of the index level over $M$ time intervals of equal size $\\Delta t$. Define $Y\\_{t,i}\\equiv e^{-r\\Delta t}V\\_{t + \\Delta t,i}$ to be the simulated continuation value for path $i$ at time $t$. We cannot use this number directly because it would imply perfect foresight. However, we can use the cross-section of all such simulated continuation values to estimate the (expected) continuation value by least-squares regression."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Given a set of basis functions $b\\_d, d=1, \\dots, D$, the continuation value is then given by the regression estimate $\\hat{C}\\_{t,i}=\\sum\\_{d=1}^{D}\\alpha\\_{d,t}^{\\*}\\cdot b\\_{d}(S\\_{t,i})$ where the optimal regression parameters $\\alpha^\\*$ are the solution of the least-squares problem stated in Least-squares regression for American option valuation."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The function `gbm_mcs_amer` implements the LSM algorithm for both American call and put options."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [64]: def gbm_mcs_amer(K, option='call'):\n             ''' Valuation of American option in Black-Scholes-Merton\n             by Monte Carlo simulation by LSM algorithm\n\n             Parameters\n             ==========\n             K : float\n                 (positive) strike price of the option\n             option : string\n                 type of the option to be valued ('call', 'put')\n\n             Returns\n             =======\n             C0 : float\n                 estimated present value of European call option\n             '''\n             dt = T / M\n             df = np.exp(-r * dt)\n             # simulation of index levels\n             S = np.zeros((M + 1, I))\n             S[0] = S0\n             sn = gen_sn(M, I)\n             for t in range(1, M + 1):\n                 S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt\n                         + sigma * np.sqrt(dt) * sn[t])\n             # case based calculation of payoff\n             if option == 'call':\n                 h = np.maximum(S - K, 0)\n             else:\n                 h = np.maximum(K - S, 0)\n             # LSM algorithm\n             V = np.copy(h)\n             for t in range(M - 1, 0, -1):\n                 reg = np.polyfit(S[t], V[t + 1] * df, 7)\n                 C = np.polyval(reg, S[t])\n                 V[t] = np.where(C > h[t], V[t + 1] * df, h[t])\n             # MCS estimator\n             C0 = df * 1 / I * np.sum(V[1])\n             return C0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [65]: gbm_mcs_amer(110., option='call')",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[65]: 7.7789332794490775",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [66]: gbm_mcs_amer(110., option='put')",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[66]: 13.614023206242392",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The European value of an option represents a lower bound to the American option’s value. The difference is generally called the _early exercise premium_. In what follows, we compare European and American option values for the same range of strikes as before to estimate the option premium. This time we take puts. [12](ch05.html#idp21721328)"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [67]: euro_res = []\n         amer_res = []\n         k_list = np.arange(80., 120.1, 5.)\n         for K in k_list:\n             euro_res.append(gbm_mcs_dyna(K, 'put'))\n             amer_res.append(gbm_mcs_amer(K, 'put'))\n         euro_res = np.array(euro_res)\n         amer_res = np.array(amer_res)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "??? shows that for the range of strikes chosen the premium can rise to up to 10%."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [68]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))\n         ax1.plot(k_list, euro_res, 'b', label='European put')\n         ax1.plot(k_list, amer_res, 'ro', label='American put')\n         ax1.set_ylabel('call option value')\n         ax1.grid(True)\n         ax1.legend(loc=0)\n         wi = 1.0\n         ax2.bar(k_list - wi / 2, (amer_res - euro_res) / euro_res * 100, wi)\n         ax2.set_xlabel('strike')\n         ax2.set_ylabel('early exercise premium in %')\n         ax2.set_xlim(left=75, right=125)\n         ax2.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"opt_euro_amer\"><img src=\"files/images/opt_euro_amer.png\" alt=\"opt euro amer\"><figcaption>Comparsion of European and LSM Monte Carlo estimator values</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Risk Measures"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In addition to valuation, _risk management_ is another important application area of stochastic methods and simulation. This section illustrates the calculation/estimation of two of the most common risk measures applied today in the finance industry."
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "Value-at-Risk"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Value-at-Risk_ (VaR) is one of the most widely used risk measures and a much debated one. Loved by practitioners for its intuitive appeal, it is widely discussed and criticized by many&ndash;mainly on theoretical grounds with regard to its limited ability to capture what is called _tail risk_. More on this shortly. In words, VaR is a number denoted in currency units (e.g. USD, EUR, JPY) indicating a loss (of a portfolio, a sinlge position, etc.) that is not exceeded with some confidence level (probability) over a given period of time."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider a stock position, worth 1 mn USD today, that has a VaR of 50.000 USD at a confidence level of 99% over a time period of 30 days (one month). Then this VaR figure says, that with a probability of 99% (i.e. in 99 out of 100 cases) the loss to be expected over a period of 30 days will _not exceed_ 50.000 USD. However, it does not say anything about the size of the loss once a loss beyond 50.000 USD occurs, i.e. if the maximum loss is 100.000 or 500.000 USD and with what probability such “higher than VaR losses” occur."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Assume again that we are in a Black-Scholes-Merton set-up and consder the following parametrization and simulation of index levels at a future data $T = 30 / 365$, i.e. we assume a period of 30 days."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [69]: S0 = 100\n         r = 0.05\n         sigma = 0.25\n         T = 30 / 365.\n         I = 10000\n         ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T\n                      + sigma * np.sqrt(T) * npr.standard_normal(I))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To estimate VaR figures, we need the simulated absolute profits and losses relative to the value of the position today in a sorted manner, i.e. from the severest loss to the largest profit."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [70]: R_gbm = np.sort(ST - S0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-18 shows the histogram of the simulated absolute performance values."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [71]: plt.hist(R_gbm, bins=50)\n         plt.xlabel('absolute return')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"var_hist_gbm\"><img src=\"files/images/var_hist_gbm.png\" alt=\"var hist gbm\"><figcaption>Absolute returns of geometric Brownian motion (30d)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Having the `ndarray` object with the sorted results, the function `scoreatpercentile` already does the trick. All we have to do is to define the percentiles (in percent values) in which we are interested in. In the `list` object `percs` , 0.1 translates into a confidence level of $100\\%-0.1\\% = 99.9\\%$. The 30 day VaR given a confidence level of 99.9% in this case is 20.2 currency units while it is 8.9 at the 90% confidence level."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [72]: percs = [0.01, 0.1, 1., 2.5, 5.0, 10.0]\n         var = scs.scoreatpercentile(R_gbm, percs)\n         print \"%16s %16s\" % ('Confidence Level', 'Value-at-Risk')\n         print 33 * \"-\"\n         for pair in zip(percs, var):\n             print \"%16.2f %16.3f\" % (100 - pair[0], -pair[1])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[72]: Confidence Level    Value-at-Risk\n         ---------------------------------\n                    99.99           26.072\n                    99.90           20.175\n                    99.00           15.753\n                    97.50           13.265\n                    95.00           11.298\n                    90.00            8.942",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As a second example, recall the jump diffusion set-up from Merton which we want to simulate dynamically."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [73]: dt = 30. / 365 / M\n         rj = lamb * (np.exp(mu + 0.5 * delta ** 2) - 1)\n         S = np.zeros((M + 1, I))\n         S[0] = S0\n         sn1 = npr.standard_normal((M + 1, I))\n         sn2 = npr.standard_normal((M + 1, I))\n         poi = npr.poisson(lamb * dt, (M + 1, I))\n         for t in range(1, M + 1, 1):\n             S[t] = S[t - 1] * (np.exp((r - rj - 0.5 * sigma ** 2) * dt\n                                + sigma * np.sqrt(dt) * sn1[t])\n                                + (np.exp(mu + delta * sn2[t]) - 1)\n                                * poi[t])\n             S[t] = np.maximum(S[t], 0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [74]: R_jd = np.sort(S[-1] - S0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In this case, with the jump component having a negative mean, we see something like a bi-modal distribution for the simulated profits/losses in Figure 5-19. From a normal distribution point of view, we have a strongly pronounced left _fat tail_."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [75]: plt.hist(R_jd, bins=50)\n         plt.xlabel('absolute return')\n         plt.ylabel('frequency')\n         plt.grid(True)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"var_hist_jd\"><img src=\"files/images/var_hist_jd.png\" alt=\"var hist jd\"><figcaption>Absolute returns of jump diffusion (30d)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For this process and parametrization, the VaR over 30 days on the 90% level is almost identical while it is more as _three times_ as high as with the geometric Brownian motion (71 vs. 20.2 currency units). This illustrates the problem of capturing the tail risk so often encountered in financial markets by the standard VaR measure."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [76]: percs = [0.01, 0.1, 1., 2.5, 5.0, 10.0]\n         var = scs.scoreatpercentile(R_jd, percs)\n         print \"%16s %16s\" % ('Confidence Level', 'Value-at-Risk')\n         print 33 * \"-\"\n         for pair in zip(percs, var):\n             print \"%16.2f %16.3f\" % (100 - pair[0], -pair[1])",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[76]: Confidence Level    Value-at-Risk\n         ---------------------------------\n                    99.99           75.029\n                    99.90           71.833\n                    99.00           55.901\n                    97.50           45.697\n                    95.00           25.993\n                    90.00            8.773",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To further illustrate the point, we lastly show the VaR measures for both cases in direct comparion graphically. As Figure 5-20 shows, the VaR measures behave completely different given a range of typical confidence level."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [77]: percs = list(np.arange(0.0, 10.1, 0.1))\n         gbm_var = scs.scoreatpercentile(R_gbm, percs)\n         jd_var = scs.scoreatpercentile(R_jd, percs)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [78]: plt.plot(percs, gbm_var, 'b', lw=1.5, label='GBM')\n         plt.plot(percs, jd_var, 'r', lw=1.5, label='JD')\n         plt.legend(loc=4)\n         plt.xlabel('100 - confidence level [%]')\n         plt.ylabel('value-at-risk')\n         plt.grid(True)\n         plt.ylim(ymax=0.0)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"var_comp\"><img src=\"files/images/var_comp.png\" alt=\"var comp\"><figcaption>Value-at-risk for geometric Brownian motion and jump diffusion</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 3,
          "metadata": {
          },
          "source": "Credit Value Adjustments"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Other important risk measures are the Credit Value-at-Risk (CVaR) and the Credit Value Adjustment (CVA) which is derived from the CVaR. Again, consider the benchmark set-up of Black-Scholes-Merton with the following parametrization."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [79]: S0 = 100.\n         r = 0.05\n         sigma = 0.2\n         T = 1.\n         I = 100000\n         ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T\n                      + sigma * np.sqrt(T) * npr.standard_normal(I))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In the most simple case, one considers a fixed (average) loss level $L$ and a fixed probability $p$ for default (per year) of a counterparty."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [80]: L = 0.5",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [81]: p = 0.01",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using the Poisson distribution, default scenarios are generated as follows, taking into account that a default can only occur once."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [82]: D = npr.poisson(p * T, I)\n         D = np.where(D > 1, 1, D)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Without default, the risk-neutral value of the future index level should be equal to the current value of the asset today (up to numerical issues)."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [83]: np.exp(-r * T) * 1 / I * np.sum(ST)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[83]: 99.981825216842537",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The CVaR under our assumptions is calculated as follows."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [84]: CVaR = np.exp(-r * T) * 1 / I * np.sum(L * D * ST)\n         CVaR",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[84]: 0.51520111341613528",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Analogously, the present value of the asset, adjusted for the credit risk, is given as follows."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [85]: S0_CVA = np.exp(-r * T) * 1 / I * np.sum((1 - L * D) * ST)\n         S0_CVA",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[85]: 99.466624103426184",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This should be (roughly) the same as subtracting the CVaR value from the current asset value."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [86]: S0_adj = S0 - CVaR\n         S0_adj",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[86]: 99.48479888658386",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In this particular simulation example, we observe roughly 1,000 losses due to credit risk which is to be expected given the assumed default probability of 1% and 100,000 simulated paths."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [87]: np.count_nonzero(L * D * ST)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[87]: 1031",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-21 shows the complete frequency distribution of the losses due to a default. Of course, in the large majority of cases there is no loss to observe (i.e. in roughly 99,000 cases)."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [88]: plt.hist(L * D * ST, bins=50)\n         plt.xlabel('loss')\n         plt.ylabel('frequency')\n         plt.grid(True)\n         plt.ylim(ymax=175)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"cva_hist_stock\"><img src=\"files/images/cva_hist_stock.png\" alt=\"cva hist stock\"><figcaption>Losses due to risk-neutrally expected default (stock)</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Consider now the case of a European call option. Its value, is about 10.4 currency units at a strike of 100."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [89]: K = 100.\n         hT = np.maximum(ST - K, 0)\n         C0 = np.exp(-r * T) * 1 / I * np.sum(hT)\n         C0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[89]: 10.427336109660063",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The CVaR is about 5 cents given the same assumptions with regard to probability of default and loss level."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [90]: CVaR = np.exp(-r * T) * 1 / I * np.sum(L * D * hT)\n         CVaR",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[90]: 0.053822578452208128",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Accordingly, the adjusted option value is roughly 5 cents lower."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [91]: C0_CVA = np.exp(-r * T) * 1 / I * np.sum((1 - L * D) * hT)\n         C0_CVA",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[91]: 10.37351353120785",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Compared to the case of a regular asset, the option case has somewhat different characteristics. We only see a little more than 500 losses due default, although we again have about 1,000 defaults. This results from the fact that the payoff of the option at maturity has a high probability of being zero."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [92]: np.count_nonzero(L * D * hT)  # number of losses",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[92]: 582",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [93]: np.count_nonzero(D)  # number of defaults",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[93]: 1031",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [94]: I - np.count_nonzero(hT)  # zero payoff",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "Out[94]: 43995",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 5-22 shows that the CVaR for the option has a completely different frequency distribution compared to the regular asset case."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "In [95]: plt.hist(L * D * hT, bins=50)\n         plt.xlabel('loss')\n         plt.ylabel('frequency')\n         plt.grid(True)\n         plt.ylim(ymax=350)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"cva_hist_opt\"><img src=\"files/images/cva_hist_opt.png\" alt=\"cva hist opt\"><figcaption>Losses due to risk-neutrally expected default (call option)</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Conclusions"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Further Reading"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "These sources might be helpful to further explore the topics of this chapter."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p>Duffie, Darrell and Kenneth Singleton (2003): Credit Risk&amp;ndash;Pricing, Measurement, and Management. Princeton University Press, Princeton.</p>\n</li>\n</ul>"
        }
      ],
      "metadata": {
      }
    }
  ]
}